{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Privthi configuration file guide\n",
        "One essential step of working with the Prithvi HLS foundation model is to set up a configuration file. This file will specify everything from the dataset file paths to training and model metaparameters. The intention of this notebook is to familiarize users with many of the important parameters that require user definition. The configuration defined in this notebook corresponds with [the HLS foundation fine-tuning guide found here](https://colab.research.google.com/drive/1pPhGt9x3p6KVXG4oIUc9y-sqPcaevFbq?usp=sharing).\n",
        "\n",
        "Before running, ensure that you have completed the well pad data generation by following [this guide](https://colab.research.google.com/drive/1VEJOpyeTJeYSX8EcINe1m7lA-Bi7YoqU?usp=sharing)."
      ],
      "metadata": {
        "id": "cZUCfNY4aSVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overview\n",
        "\n",
        "Each cell within this notebook corresponds to a section of the config file. Each time you run a cell, the code within that section is appended to the file named hls_config.py which is stored in colab's local storage. It is recommended not to run a cell more than once. Each cell will have a title and some will have a brief description."
      ],
      "metadata": {
        "id": "dnmAzQ6CcBnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imports and Setup"
      ],
      "metadata": {
        "id": "HHde-iimdw6g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gB_CIICSvtR",
        "outputId": "ffa0d5a9-26d3-4a28-e2c0-27da50dee65e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hls_config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a hls_config.py\n",
        "import os\n",
        "custom_imports = dict(imports=['geospatial_fm'])\n",
        "\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "load_from = None\n",
        "resume_from = None\n",
        "cudnn_benchmark = True\n",
        "\n",
        "dataset_type = 'GeospatialDataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Root\n",
        "The `data_root` parameter specifies the path to the dataset folder. This folder should contain a training and validation folder. Each of these folders should contain both the mask and image tiles used for training/validation."
      ],
      "metadata": {
        "id": "uQnhb4A8d4wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "data_root = '/content/gdrive/MyDrive/HLS foundation guide/dataset'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBNmvBnKT9Um",
        "outputId": "f1a05472-2df9-45c3-89a6-67b265037aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GPU settings and Image Specs"
      ],
      "metadata": {
        "id": "iidewqP2eaSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "num_frames = 1\n",
        "img_size = 224\n",
        "num_workers = 4\n",
        "samples_per_gpu = 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYc8Dpm7V4Oz",
        "outputId": "fad30cdd-2535-4db9-b92f-976791355e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mean and Standard Deviation\n",
        "This is the mean and standard deviation of the pixel values for each band within the whole training set. This information is useful for normalization."
      ],
      "metadata": {
        "id": "A8yBiNIGe6ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "img_norm_cfg = dict(\n",
        "    means=[0.08285756, 0.09015894, 0.14294316, 0.26300612, 0.25897524,\n",
        "       0.21503   ],\n",
        "    stds=[0.03269446, 0.03606708, 0.04953382, 0.07931096, 0.06092547,\n",
        "       0.05726682])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmP8JuVgV-8H",
        "outputId": "bc8a5ebc-afc6-4981-edaf-07c9bcf31e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Image Settings\n",
        "\n",
        "* `bands` specifies which bands from the training data will be passed into the model during training. The bands that the HLS Privthi model is pre-trained on are [Blue, Green, Red, Narrow NIR, SWIR1, and SWIR2].\n",
        "\n",
        "* `img_suffix` and `seg_map_suffix` are used to distinguish image tiles from mask tiles. These are the naming conventions used when generating the data.\n",
        "\n",
        "* `orig_nsize` is the pixel width and height of your tiles.\n",
        "\n",
        "* `tile_size` are what the tiles are resized to before being passed into the model. It is suggested not to change this value."
      ],
      "metadata": {
        "id": "f_w-u0JWfOc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "bands = [0, 1, 2, 3, 4, 5]\n",
        "img_suffix = '_tile.tif'\n",
        "seg_map_suffix = '_mask.tif'\n",
        "orig_nsize = 256\n",
        "tile_size = 224\n",
        "\n",
        "crop_size = (tile_size, tile_size)\n",
        "ignore_index = -1\n",
        "image_nodata = -9999\n",
        "image_nodata_replace = 0\n",
        "image_to_float32 = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSCfwWmMWNXF",
        "outputId": "2f222583-1641-47d8-ac6d-fd877b0b5496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment name and file path.\n",
        "\n",
        "The `experiment` variable is the name of the project and `project_dir` is the location where all of the files associated with fine-tuning will be saved. Model checkpoints will be saved here."
      ],
      "metadata": {
        "id": "_cdYFmROhQWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "experiment = 'HLS_wellpad_finetuning'\n",
        "project_dir = '/content/gdrive/MyDrive/HLS foundation guide'\n",
        "\n",
        "work_dir = os.path.join(project_dir, experiment)\n",
        "save_path = work_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J66J7gMWqR5",
        "outputId": "f5ab45a0-597b-49a9-e4a2-2869c453ebaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Parameters and Pre-trained Model path\n",
        "The `pretrained_weights_path` specifies the location of the pre-trained HLS Prithvi model. Before fine tuning, you must download the Prithvi_100m.pt checkpoint from [this repo](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M).\n",
        "\n",
        "The `epochs` and `eval_epoc_interval` specifies how many epochs to train for and when to evaluate model performance on the validation set. For the purpose of this guide, we will train for only 5 epochs and evaluate after each epoch."
      ],
      "metadata": {
        "id": "tcR3rywLjmck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "pretrained_weights_path =  '/content/gdrive/MyDrive/HLS Foundation Model/Prithvi_100M.pt'\n",
        "epochs=5\n",
        "eval_epoch_interval = 1\n",
        "\n",
        "\n",
        "num_layers = 12\n",
        "patch_size = 16\n",
        "embed_dim = 768\n",
        "num_heads = 12\n",
        "tubelet_size = 1\n",
        "output_embed_dim = num_frames*embed_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBO6tg7YWgHp",
        "outputId": "20e3f2a4-1493-4e89-b076-f80ff428cf0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training, Validation, and Testing pipelines\n",
        "These lists specify the on-the-fly preprocessing steps that are applied to the data during training, testing, and validation"
      ],
      "metadata": {
        "id": "xuTwEyhBniSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "train_pipeline = [\n",
        "    dict(type='LoadGeospatialImageFromFile', to_float32=image_to_float32),\n",
        "    dict(type='LoadGeospatialAnnotations', reduce_zero_label=False),\n",
        "    dict(type='BandsExtract', bands=bands),\n",
        "    dict(type='RandomFlip', prob=0.5),\n",
        "    dict(type='ToTensor', keys=['img', 'gt_semantic_seg']),\n",
        "    # to channels first\n",
        "    dict(type=\"TorchPermute\", keys=[\"img\"], order=(2, 0, 1)),\n",
        "    dict(type='TorchNormalize', **img_norm_cfg),\n",
        "    dict(type='TorchRandomCrop', crop_size=(tile_size, tile_size)),\n",
        "    dict(type='Reshape', keys=['img'], new_shape=(len(bands), num_frames, tile_size, tile_size)),\n",
        "    dict(type='Reshape', keys=['gt_semantic_seg'], new_shape=(1, tile_size, tile_size)),\n",
        "    dict(\n",
        "        type='CastTensor',\n",
        "        keys=['gt_semantic_seg'],\n",
        "        new_type='torch.LongTensor'),\n",
        "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
        "]\n",
        "test_pipeline = [\n",
        "    dict(type='LoadGeospatialImageFromFile', to_float32=image_to_float32),\n",
        "    dict(type='BandsExtract', bands=bands),\n",
        "    dict(type='ToTensor', keys=['img']),\n",
        "    # to channels first\n",
        "    dict(type=\"TorchPermute\", keys=[\"img\"], order=(2, 0, 1)),\n",
        "    dict(type='TorchNormalize', **img_norm_cfg),\n",
        "    dict(\n",
        "        type='Reshape',\n",
        "        keys=['img'],\n",
        "        new_shape=(len(bands), num_frames, -1, -1),\n",
        "        look_up=dict({\n",
        "            '2': 1,\n",
        "            '3': 2\n",
        "        })),\n",
        "    dict(type='CastTensor', keys=['img'], new_type='torch.FloatTensor'),\n",
        "    dict(\n",
        "        type='CollectTestList',\n",
        "        keys=['img'],\n",
        "        meta_keys=[\n",
        "            'img_info', 'seg_fields', 'img_prefix', 'seg_prefix', 'filename',\n",
        "            'ori_filename', 'img', 'img_shape', 'ori_shape', 'pad_shape',\n",
        "            'scale_factor', 'img_norm_cfg'\n",
        "        ])\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4uHIKXFW8n3",
        "outputId": "7513fbf5-d8ea-459e-8ecd-18a08e27fb88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training, Testing, and Validation split information\n",
        "These dictionaries specify several pieces of information that relate to the various dataset splits including folder locations, number of classes, and desired pipeline. For the purposes of this guideline, the `validation` and `test` sets source their data from the same folder."
      ],
      "metadata": {
        "id": "OZw4BJGyoGvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "CLASSES = ('Not Well Pad', 'Well Pad')\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=samples_per_gpu,\n",
        "    workers_per_gpu=num_workers,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        CLASSES=CLASSES,\n",
        "        data_root=data_root,\n",
        "        img_dir='training',\n",
        "        ann_dir='training',\n",
        "        img_suffix=img_suffix,\n",
        "        seg_map_suffix=seg_map_suffix,\n",
        "        pipeline=train_pipeline,\n",
        "        ignore_index=-1),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        CLASSES=CLASSES,\n",
        "        data_root=data_root,\n",
        "        img_dir='validation',\n",
        "        ann_dir='validation',\n",
        "        img_suffix=img_suffix,\n",
        "        seg_map_suffix=seg_map_suffix,\n",
        "        pipeline=test_pipeline,\n",
        "        ignore_index=-1),\n",
        "    test=dict(\n",
        "        type=dataset_type,\n",
        "        CLASSES=CLASSES,\n",
        "        data_root=data_root,\n",
        "        img_dir='validation',\n",
        "        ann_dir='validation',\n",
        "        img_suffix=img_suffix,\n",
        "        seg_map_suffix=seg_map_suffix,\n",
        "        pipeline=test_pipeline,\n",
        "        ignore_index=-1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee15LH87XBX2",
        "outputId": "1851fcf3-5b9b-4ddc-c738-80f79cbd7eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Meta-Parameters\n",
        "This sections specifies other important metaparameters that are relavent for training including the loss function, optimizer, and learning rate."
      ],
      "metadata": {
        "id": "cT1SYClnpDhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "optimizer = dict(type='Adam', lr=1.3e-05, betas=(0.9, 0.999))\n",
        "optimizer_config = dict(grad_clip=None)\n",
        "lr_config = dict(\n",
        "    policy='poly',\n",
        "    warmup='linear',\n",
        "    warmup_iters=1500,\n",
        "    warmup_ratio=1e-06,\n",
        "    power=1.0,\n",
        "    min_lr=0.0,\n",
        "    by_epoch=False)\n",
        "log_config = dict(\n",
        "    interval=20,\n",
        "    hooks=[\n",
        "        dict(type='TextLoggerHook', by_epoch=False),\n",
        "        dict(type='TensorboardLoggerHook', by_epoch=False)\n",
        "    ])\n",
        "checkpoint_config = dict(\n",
        "    by_epoch=True, interval=10, out_dir=save_path\n",
        ")\n",
        "\n",
        "evaluation = dict(\n",
        "    interval=eval_epoch_interval, metric=\"mIoU\", pre_eval=True, save_best=\"mIoU\", by_epoch=True\n",
        ")\n",
        "\n",
        "runner = dict(type=\"EpochBasedRunner\", max_epochs=epochs)\n",
        "\n",
        "loss_func=dict(\n",
        "            type='DiceLoss', use_sigmoid=False, loss_weight=1,\n",
        "            ignore_index=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l38uFp1XHbL",
        "outputId": "1c7f307f-ebea-4390-95ed-17bf592451a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fine-Tune Model Architecture\n",
        "\n",
        "In order to fine-tune the HLS Prithvi model, several layers are added to the model to allow for image segmentation to occur. Here is where those segmentation layers are defined"
      ],
      "metadata": {
        "id": "E1AZLdYXpHCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a hls_config.py\n",
        "workflow = [('train', 1)]\n",
        "norm_cfg = dict(type='BN', requires_grad=True)\n",
        "model = dict(\n",
        "    type='TemporalEncoderDecoder',\n",
        "    frozen_backbone=False,\n",
        "    backbone=dict(\n",
        "        type='TemporalViTEncoder',\n",
        "        pretrained=pretrained_weights_path,\n",
        "        img_size=img_size,\n",
        "        patch_size=patch_size,\n",
        "        num_frames=num_frames,\n",
        "        tubelet_size=tubelet_size,\n",
        "        in_chans=len(bands),\n",
        "        embed_dim=embed_dim,\n",
        "        depth=12,\n",
        "        num_heads=num_heads,\n",
        "        mlp_ratio=4.0,\n",
        "        norm_pix_loss=False),\n",
        "    neck=dict(\n",
        "        type='ConvTransformerTokensToEmbeddingNeck',\n",
        "        embed_dim=embed_dim*num_frames,\n",
        "        output_embed_dim=output_embed_dim,\n",
        "        drop_cls_token=True,\n",
        "        Hp=14,\n",
        "        Wp=14),\n",
        "    decode_head=dict(\n",
        "        num_classes=len(CLASSES),\n",
        "        in_channels=output_embed_dim,\n",
        "        type='FCNHead',\n",
        "        in_index=-1,\n",
        "        channels=256,\n",
        "        num_convs=1,\n",
        "        concat_input=False,\n",
        "        dropout_ratio=0.1,\n",
        "        norm_cfg=dict(type='BN', requires_grad=True),\n",
        "        align_corners=False,\n",
        "        loss_decode=loss_func),\n",
        "    auxiliary_head=dict(\n",
        "        num_classes=len(CLASSES),\n",
        "        in_channels=output_embed_dim,\n",
        "        type='FCNHead',\n",
        "        in_index=-1,\n",
        "        channels=256,\n",
        "        num_convs=2,\n",
        "        concat_input=False,\n",
        "        dropout_ratio=0.1,\n",
        "        norm_cfg=dict(type='BN', requires_grad=True),\n",
        "        align_corners=False,\n",
        "        loss_decode=loss_func),\n",
        "    train_cfg=dict(),\n",
        "    test_cfg=dict(mode='slide', stride=(int(tile_size/2), int(tile_size/2)), crop_size=(tile_size, tile_size)))\n",
        "gpu_ids = range(0, 1)\n",
        "auto_resume = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNcuXGfeXOjd",
        "outputId": "1c12a1c9-6f3a-41fe-f15f-181dd02199a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to hls_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Connect to Google Drive and save the config file in the desired location."
      ],
      "metadata": {
        "id": "hYqLCIT0cItN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEtUvRhircL6",
        "outputId": "3579ecb0-c77d-4f8c-a0f4-ca9f533fb15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = '/content/hls_config.py'\n",
        "destination_folder = '/content/gdrive/MyDrive/HLS foundation guide'\n",
        "\n",
        "shutil.move(source_folder, destination_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TZatKZ_mrIz2",
        "outputId": "b86e892e-0152-4f69-fbb9-ccf50555c6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/HLS foundation guide/hls_config.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}